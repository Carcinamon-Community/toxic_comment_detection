{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1nPkemWDtm_"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_WORDS = 30000\n",
        "EMBEDDING_DIM = 100\n",
        "MAXLEN = 100\n",
        "PADDING = 'post'\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "TRAIN_FILE = \"processed_train.csv\"\n",
        "TEST_FILE = \"processed_test.csv\"\n",
        "STOPWORDS_FILE = \"stopwordbahasa.csv\"\n",
        "STOPWORDS = []\n",
        "num_classes = 4\n",
        "learning_rate = 0.001\n",
        "batch_size = 16\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "Le4Bq4JkDw1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_stopwords(stopwords_file=STOPWORDS_FILE):\n",
        "    \"\"\"\n",
        "    Loads the stopwords from the stopwords file\n",
        "    \n",
        "    Args:\n",
        "        stopwords_file (string): path to the stopwords file\n",
        "    Returns:\n",
        "        stopwords (list): list of stopwords\n",
        "    \"\"\"\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/stopwordbahasa.csv', 'r') as f:\n",
        "        stopwords = []\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            stopwords.append(row[0])\n",
        "        \n",
        "        return stopwords\n",
        "\n",
        "STOPWORDS = load_stopwords(STOPWORDS_FILE)"
      ],
      "metadata": {
        "id": "5G7xnkabgpPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebu2npb6D5f4",
        "outputId": "5a4b9c24-d4a4-4a0b-d94c-f4df0ae15357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ada', 'adalah', 'adanya', 'adapun', 'agak']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(sentence, stopwords=STOPWORDS):\n",
        "    \"\"\"\n",
        "    Removes a list of stopwords\n",
        "    \n",
        "    Args:\n",
        "        sentence (string): sentence to remove the stopwords from\n",
        "        stopwords (list): list of stopwords to remove from the sentence\n",
        "        \n",
        "    Returns:\n",
        "        sentence (string): lowercase sentence without the stopwords\n",
        "    \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    no_stopwords = [word for word in words if word not in stopwords]\n",
        "    sentence = \" \".join(no_stopwords)\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "KQf6Z8aEEXeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data_from_file(filename):\n",
        "    \"\"\"\n",
        "    Extracts sentences and labels from a CSV file\n",
        "    \n",
        "    Args:\n",
        "        filename (string): path to the CSV file\n",
        "    \n",
        "    Returns:\n",
        "        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    with open(filename, 'r', encoding=\"utf8\") as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            toxicity = int(row[2]) + int(row[3]) + int(row[4]) + int(row[5])\n",
        "            if toxicity > 0:\n",
        "                labels.append(1)\n",
        "            else:\n",
        "                labels.append(0)\n",
        "            sentence = row[6]\n",
        "            sentence = remove_stopwords(sentence)\n",
        "            sentences.append(sentence)\n",
        "\n",
        "    return sentences, np.array(labels)"
      ],
      "metadata": {
        "id": "1fpxHPmjEl9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences, test_labels = parse_data_from_file(\"/content/drive/MyDrive/Colab Notebooks/processed_test.csv\")\n",
        "train_sentences, train_labels  = parse_data_from_file(\"/content/drive/MyDrive/Colab Notebooks/processed_train.csv\")"
      ],
      "metadata": {
        "id": "CBUvbUxBEqHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_sentences[0])\n",
        "print(train_labels[0])\n",
        "\n",
        "print(test_sentences[0])\n",
        "print(test_labels[0])\n",
        "\n",
        "# number of sentences in the training set\n",
        "print(f\"No. of sentences in the training set: {len(train_sentences)}\")\n",
        "\n",
        "# number of sentences in the test set\n",
        "print(f\"No. of sentences in the test set: {len(test_sentences)}\")\n",
        "\n",
        "# shape of the labels in the training set\n",
        "print(f\"Shape of the labels in the training set: {train_labels.shape}\")\n",
        "\n",
        "# shape of the labels in the test set\n",
        "print(f\"Shape of the labels in the test set: {test_labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCZnyOfrcd9i",
        "outputId": "96b1ad44-7c1d-45ff-e4fa-b6f393a4a1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jabar provinsi barokah nwoi anjing bodoh npropinsi ngerusak ngebakar gereja jatim provinsi lumbung nasbung jatim npropinsi penghasil gembong teroris jateng lumbung nasbung jateng ngarang stereotip piliah pilih nbangsat pecun\n",
            "1\n",
            "spanduk prof video orasi buku lainlain sngat heran rkyat tolol jls niat hizbut tahrir tegakan khilafah ganti negara kesatuan republik indonesia berkilah tegakan khilafah munafik apanya khilafah\n",
            "1\n",
            "No. of sentences in the training set: 6995\n",
            "No. of sentences in the test set: 778\n",
            "Shape of the labels in the training set: (6995,)\n",
            "Shape of the labels in the test set: (778,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_tokenizer(train_sentences, num_words, oov_token):\n",
        "    \"\"\"\n",
        "    Instantiates the Tokenizer class on the training sentences\n",
        "    \n",
        "    Args:\n",
        "        train_sentences (list of string): lower-cased sentences without stopwords to be used for training\n",
        "        num_words (int) - number of words to keep when tokenizing\n",
        "        oov_token (string) - symbol for the out-of-vocabulary token\n",
        "    \n",
        "    Returns:\n",
        "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    \n",
        "    # Instantiate the Tokenizer class, passing in the correct values for num_words and oov_token\n",
        "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
        "    \n",
        "    # Fit the tokenizer to the training sentences\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "ZDvtB8EiE3kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function\n",
        "tokenizer = fit_tokenizer(train_sentences, NUM_WORDS, OOV_TOKEN)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
        "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ABq4iiLE-_4",
        "outputId": "a4c33a35-92e5-4012-f522-c8c6654bd1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary contains 28899 words\n",
            "\n",
            "<OOV> token included in vocabulary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_and_pad(sentences, tokenizer, padding, maxlen):\n",
        "    \"\"\"\n",
        "    Generates an array of token sequences and pads them to the same length\n",
        "    \n",
        "    Args:\n",
        "        sentences (list of string): list of sentences to tokenize and pad\n",
        "        tokenizer (object): Tokenizer instance containing the word-index dictionary\n",
        "        padding (string): type of padding to use\n",
        "        maxlen (int): maximum length of the token sequence\n",
        "    \n",
        "    Returns:\n",
        "        padded_sequences (array of int): tokenized sentences padded to the same length\n",
        "    \"\"\" \n",
        "    \n",
        "    ### START CODE HERE\n",
        "       \n",
        "    # Convert sentences to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    \n",
        "    # Pad the sequences using the correct padding and maxlen\n",
        "    padded_sequences = pad_sequences(sequences, padding=padding, maxlen=maxlen)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return padded_sequences"
      ],
      "metadata": {
        "id": "fVb9uFxaFDP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "train_padded_seq = seq_and_pad(train_sentences, tokenizer, PADDING, MAXLEN)\n",
        "test_padded_seq = seq_and_pad(test_sentences, tokenizer, PADDING, MAXLEN)\n",
        "\n",
        "print(f\"Padded training sequences have shape: {train_padded_seq.shape}\\n\")\n",
        "print(f\"Padded validation sequences have shape: {test_padded_seq.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV1oZlzkFHco",
        "outputId": "fb2d1247-1d4c-4b64-f0a3-6746a7f875b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded training sequences have shape: (6995, 100)\n",
            "\n",
            "Padded validation sequences have shape: (778, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_words, embedding_dim, maxlen):\n",
        "    tf.random.set_seed(123)\n",
        "    \n",
        "    model = tf.keras.Sequential([ \n",
        "    tf.keras.layers.Embedding(NUM_WORDS, EMBEDDING_DIM, input_length=MAXLEN),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")  \n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Ids4YbANFKpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)\n",
        "\n",
        "history = model.fit(train_padded_seq, train_labels, epochs=10, validation_data=(test_padded_seq, test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGfeTZlOFNkI",
        "outputId": "034877da-4bf1-430b-e9b8-a32ad16b7c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "219/219 [==============================] - 25s 39ms/step - loss: 0.6740 - accuracy: 0.5758 - precision_4: 0.5739 - recall_4: 0.9511 - val_loss: 0.5911 - val_accuracy: 0.7828 - val_precision_4: 0.8583 - val_recall_4: 0.7292\n",
            "Epoch 2/10\n",
            "219/219 [==============================] - 9s 40ms/step - loss: 0.3988 - accuracy: 0.8302 - precision_4: 0.8363 - recall_4: 0.8674 - val_loss: 0.4035 - val_accuracy: 0.8072 - val_precision_4: 0.8373 - val_recall_4: 0.8102\n",
            "Epoch 3/10\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.2170 - accuracy: 0.9182 - precision_4: 0.9195 - recall_4: 0.9364 - val_loss: 0.4753 - val_accuracy: 0.7918 - val_precision_4: 0.8771 - val_recall_4: 0.7269\n",
            "Epoch 4/10\n",
            "219/219 [==============================] - 8s 37ms/step - loss: 0.1363 - accuracy: 0.9558 - precision_4: 0.9578 - recall_4: 0.9639 - val_loss: 0.5031 - val_accuracy: 0.8033 - val_precision_4: 0.8093 - val_recall_4: 0.8449\n",
            "Epoch 5/10\n",
            "219/219 [==============================] - 9s 39ms/step - loss: 0.0789 - accuracy: 0.9758 - precision_4: 0.9755 - recall_4: 0.9817 - val_loss: 0.5719 - val_accuracy: 0.7892 - val_precision_4: 0.8268 - val_recall_4: 0.7847\n",
            "Epoch 6/10\n",
            "219/219 [==============================] - 8s 35ms/step - loss: 0.0452 - accuracy: 0.9871 - precision_4: 0.9868 - recall_4: 0.9903 - val_loss: 0.7215 - val_accuracy: 0.7763 - val_precision_4: 0.8308 - val_recall_4: 0.7500\n",
            "Epoch 7/10\n",
            "219/219 [==============================] - 9s 40ms/step - loss: 0.0308 - accuracy: 0.9921 - precision_4: 0.9916 - recall_4: 0.9944 - val_loss: 0.7879 - val_accuracy: 0.7815 - val_precision_4: 0.8018 - val_recall_4: 0.8056\n",
            "Epoch 8/10\n",
            "219/219 [==============================] - 8s 35ms/step - loss: 0.0261 - accuracy: 0.9931 - precision_4: 0.9929 - recall_4: 0.9949 - val_loss: 0.8655 - val_accuracy: 0.7763 - val_precision_4: 0.7699 - val_recall_4: 0.8519\n",
            "Epoch 9/10\n",
            "219/219 [==============================] - 9s 39ms/step - loss: 0.0203 - accuracy: 0.9946 - precision_4: 0.9949 - recall_4: 0.9954 - val_loss: 0.9894 - val_accuracy: 0.7841 - val_precision_4: 0.7773 - val_recall_4: 0.8565\n",
            "Epoch 10/10\n",
            "219/219 [==============================] - 8s 38ms/step - loss: 0.0162 - accuracy: 0.9953 - precision_4: 0.9952 - recall_4: 0.9964 - val_loss: 1.0542 - val_accuracy: 0.7751 - val_precision_4: 0.7545 - val_recall_4: 0.8819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history[f'val_{metric}'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, f'val_{metric}'])\n",
        "    plt.show()\n",
        "    \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "-8-fQ81JSd9n",
        "outputId": "db02fed0-1402-4e42-aabc-3673d892f9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1329d2ff16ec>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplot_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    }
  ]
}